# Fashion Data Engine â€” Agent Guide

This document is the primary reference for AI coding agents (OpenAI Codex, Claude Code, etc.)
working on this project. Read this before writing any code.

---

## Project Overview

A fashion data platform that:
1. **Aggregates** sales channels (í¸ì§‘ìƒµ / multi-brand edit shops) from a curated list
2. **Maps** which brands each channel carries (via web scraping)
3. **Tracks** prices, new arrivals, and sales across channels
4. **Archives** AI-generated lookbooks with virtual models (future phase)

**Current phase:** Phase 1 â€” Data Engine (channel preprocessing + brand mapping)

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Language | Python 3.12 |
| Package manager | `uv` (not pip, not poetry) |
| Database | SQLite (local dev) / PostgreSQL (production) |
| ORM | SQLAlchemy 2.0 async + Alembic migrations |
| API | FastAPI (async) |
| Crawler | Playwright (headless Chromium) + BeautifulSoup4 |
| Validation | Pydantic v2 |
| CLI | Typer + Rich |

---

## Repository Structure

```
fashion-data-engine/
â”œâ”€â”€ AGENTS.md              â† YOU ARE HERE
â”œâ”€â”€ CLAUDE.md              â† Claude Code specific context
â”œâ”€â”€ pyproject.toml         â† uv project + dependencies
â”œâ”€â”€ .env                   â† local config (not committed)
â”œâ”€â”€ .env.example           â† env template
â”œâ”€â”€ docker-compose.yml     â† PostgreSQL for production use
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ channels_input.csv    â† raw channel URLs (user-provided)
â”‚   â”œâ”€â”€ channels_cleaned.csv  â† preprocessed channel data
â”‚   â”œâ”€â”€ channels_flagged.csv  â† URLs needing manual review
â”‚   â””â”€â”€ fashion.db            â† SQLite database (gitignored)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_channels.py  â† Step 1: normalize URLs, dedup
â”‚   â”œâ”€â”€ seed_channels.py        â† Step 2: load cleaned CSV â†’ DB
â”‚   â””â”€â”€ crawl_brands.py         â† Step 3: crawl brands per channel
â”‚
â””â”€â”€ src/fashion_engine/
    â”œâ”€â”€ config.py           â† pydantic-settings from .env
    â”œâ”€â”€ database.py         â† SQLAlchemy engine + session + init_db()
    â”œâ”€â”€ cli.py              â† Typer CLI entrypoint
    â”‚
    â”œâ”€â”€ models/             â† SQLAlchemy ORM models
    â”‚   â”œâ”€â”€ __init__.py     â† IMPORTANT: imports all models (mapper registration)
    â”‚   â”œâ”€â”€ channel.py      â† Channel (íŒë§¤ì±„ë„)
    â”‚   â”œâ”€â”€ brand.py        â† Brand
    â”‚   â”œâ”€â”€ channel_brand.py â† Channelâ†”Brand N:M join table
    â”‚   â”œâ”€â”€ product.py      â† Product
    â”‚   â”œâ”€â”€ price_history.py â† PriceHistory
    â”‚   â””â”€â”€ category.py     â† Category (hierarchical, parent_id)
    â”‚
    â”œâ”€â”€ crawler/
    â”‚   â”œâ”€â”€ base.py         â† BaseCrawler (Playwright context manager)
    â”‚   â”œâ”€â”€ url_normalizer.py â† product URL â†’ homepage URL
    â”‚   â””â”€â”€ brand_crawler.py  â† extract brand list per channel
    â”‚
    â”œâ”€â”€ services/
    â”‚   â”œâ”€â”€ channel_service.py  â† DB queries for channels
    â”‚   â””â”€â”€ brand_service.py    â† DB queries for brands
    â”‚
    â””â”€â”€ api/
        â”œâ”€â”€ main.py         â† FastAPI app + lifespan
        â”œâ”€â”€ schemas.py      â† Pydantic response schemas
        â”œâ”€â”€ channels.py     â† /channels routes
        â””â”€â”€ brands.py       â† /brands routes
```

---

## Database Schema

```
channels:       id, name, url (homepage, UNIQUE), original_url, channel_type,
                country, description, is_active, created_at, updated_at

brands:         id, name, slug (UNIQUE), name_ko, origin_country,
                description, official_url, created_at

channel_brands: channel_id FK, brand_id FK  [composite PK]
                crawled_at

categories:     id, name, name_en, slug (UNIQUE), parent_id (self-ref FK),
                level (0=top, 1=mid, 2=leaf), created_at

products:       id, channel_id FK, brand_id FK, category_id FK,
                name, sku, url (UNIQUE), image_url, description,
                is_active, is_new, is_sale, created_at, updated_at

price_history:  id, product_id FK, price, original_price, currency,
                is_sale, discount_rate, crawled_at
```

**Critical:** Always import `fashion_engine.models` (the package) before running
any SQLAlchemy queries to ensure all mappers are registered. See `database.py:init_db()`.

---

## Key API Endpoints

```
GET  /                          â†’ health + version
GET  /channels/                 â†’ list all active channels
GET  /channels/{id}             â†’ channel detail
GET  /channels/{id}/brands      â†’ brands carried by this channel
GET  /brands/                   â†’ list all brands
GET  /brands/search?q=          â†’ search brands by name (Korean/English)
GET  /brands/{slug}             â†’ brand detail
GET  /brands/{slug}/channels    â†’ channels that carry this brand â† CORE FEATURE
GET  /health
```

---

## Development Workflow

### Setup
```bash
# 1. Install uv (if not installed)
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.local/bin/env

# 2. Install dependencies + Python 3.12
uv sync
uv run playwright install chromium

# 3. Copy .env
cp .env.example .env

# 4. (Optional) PostgreSQL via Docker
docker-compose up -d
# Then update .env: DATABASE_URL=postgresql+asyncpg://fashion:password@localhost:5432/fashion_db
```

### Data Pipeline (run in order)
```bash
# Step 1: preprocess raw channel URLs
uv run python scripts/preprocess_channels.py
# â†’ data/channels_cleaned.csv (154 channels as of 2026-02)

# Step 2: load into DB
uv run python scripts/seed_channels.py

# Step 3: crawl brands per channel
uv run python scripts/crawl_brands.py --limit 5   # test first
uv run python scripts/crawl_brands.py             # full run
```

### Run API
```bash
uv run uvicorn fashion_engine.api.main:app --reload
# Swagger UI: http://localhost:8000/docs
```

### CLI
```bash
uv run python -m fashion_engine.cli channels
uv run python -m fashion_engine.cli brands
uv run python -m fashion_engine.cli brand <slug>
uv run python -m fashion_engine.cli search <query>
```

### Run Tests
```bash
uv run pytest
```

---

## Coding Conventions

1. **Async-first**: all DB operations and crawler methods are `async`
2. **Services layer**: DB logic lives in `services/`, not in routes or scripts
3. **Models `__init__.py`**: always add new models to the imports list there
4. **URL normalization**: always use `url_normalizer.normalize_to_homepage()` when storing channel URLs
5. **Brand slugs**: always use `python-slugify` for slug generation â€” never hand-roll
6. **Rate limiting**: `BaseCrawler` handles delays; do not add extra `sleep()` calls outside it
7. **No docstrings on unchanged code**: only add comments where logic is non-obvious

---

## Current Status (2026-02-25)

### Done âœ…
- Project scaffolding (uv, pyproject.toml, docker-compose)
- Channel preprocessing pipeline (URL normalization, dedup, flag filtering)
- SQLAlchemy models: Channel, Brand, ChannelBrand, Category, Product, PriceHistory
- FastAPI REST API with channel/brand query endpoints
- Playwright-based brand crawler (generic + per-channel strategies)
- CLI tool (Typer + Rich)
- 154 channels loaded into DB

### Todo ğŸ”œ
- [ ] **Phase 1 completion**: Run brand crawler on all 154 channels; store brand-channel mappings
- [ ] **Channel name cleanup**: Many channel names extracted from page titles need manual curation
  - Edit `data/channels_cleaned.csv`, then re-run `seed_channels.py`
- [ ] **Channel type classification**: Most channels are `unknown` type; add more domain mappings
  in `crawler/url_normalizer.py::classify_channel_type()`
- [ ] **Country detection**: Improve `preprocess_channels.py::guess_country()` for .com domains

### Phase 2 (Price tracking)
- [ ] Product crawler (price, image, SKU per product page)
- [ ] Price history recording
- [ ] Sale / new arrival detection
- [ ] Crawl scheduler (APScheduler)

### Phase 3 (Frontend + AI)
- [ ] Next.js frontend (brand/channel search, price comparison UI)
- [ ] AI virtual try-on lookbook (Stable Diffusion or external API)
- [ ] Category tree auto-generation from crawled product data

---

## Important Notes for Agents

- **Do not commit `data/fashion.db`** (gitignored â€” it's local state)
- **Do not commit `.env`** (gitignored â€” contains secrets)
- **Do not modify `data/channels_input.csv`** without user confirmation
- When adding new crawler strategies to `brand_crawler.py::CHANNEL_STRATEGIES`,
  test with `--limit 1` before running on all channels
- The `spigen.co.kr` and `mercari.com` entries in `channels_cleaned.csv` are
  flagged as `non-fashion` / `secondhand-marketplace` â€” handle appropriately
